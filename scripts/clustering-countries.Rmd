---
title: "Clustering Analysis on Country Data"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

```{=html}
<style>
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
</style>
```

```{r, include=FALSE}
# set code chunk options
knitr::opts_chunk$set(echo = TRUE, 
                      fig.align = 'center',
                      cache = TRUE)

# allow scrolling for long code
options(width = 200)
```

# Introduction

Clustering is a type of unsupervised learning method of machine learning. In the unsupervised learning method, the inferences are drawn from the data sets which do not contain labelled output variable. It is an exploratory data analysis technique that allows us to analyze the multivariate data sets. More specifically, clustering is a task of dividing the data sets into a certain number of clusters in such a manner that the data points belonging to a cluster have similar characteristics. It is done to segregate the groups with similar traits such that the distance between the data points within the clusters is minimal. In other words, the clusters are regions where the density of similar data points is high. It is generally used for the analysis of the data set, to find insightful data among huge data sets and draw inferences from it. 

It depends on the type of algorithm we use which decides how the clusters will be created. The inferences that need to be drawn from the data sets also depend upon the user as there is no criterion for good clustering. Clustering itself can be categorized into two types. Hard Clustering and Soft Clustering. In hard clustering, one data point can belong to one cluster only. But in soft clustering, the output provided is a probability likelihood of a data point belonging to each of the pre-defined numbers of clusters. Here are several examples of different types of clustering methods: Density-Based Clustering, Hierarchical Clustering, Fuzzy Clustering, Grid-Based Clustering, and so on. In this project, we will mainly focus on three on them: Hierarchical Clustering, Model-Based Clustering, and Density-Based Clustering. 

The data set we are going to use in three clustering methods is a country-level data set that includes 167 unique country observations. The variables included in the data set are child_mort (Death of children under 5 years of age per 1000 live births), exports (Exports of goods and services per capital. Given as %age of the GDP per capital), health (Total health spending per capital. Given as %age of GDP per capital), imports (Imports of goods and services per capital. Given as %age of the GDP per capital), income (Net income per person), inflation (The measurement of the annual growth rate of the Total GDP), life_expec (The average number of years a new born child would live if the current mortality patterns are to remain the same), total_fer (The number of children that would be born to each woman if the current age-fertility rates remain the same), gdpp (The GDP per capital. Calculated as the Total GDP divided by the total population.) Since there is no labelled output, and all the predictor variables are quantitative so that we could compute the distance between each observation and therefore we could use clustering methods to analyze this data set.


**Objective:** We want to categorize countries using socio-economic and health factors that determine the overall development of each country (from the [Kaggle description](https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data)).

We load in the necessary packages and the [country data](https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data).

```{r, message=FALSE}
# load packages
library(tidyverse)
library(factoextra)
library(cluster)
library(dendextend)
library(mclust)
library(dbscan)

# read data
countries <- read.csv("../data/country-data.csv")

# use the countries as the row names
df <- countries %>%
  column_to_rownames(var = "country")

# show the first few rows of the dataframe
head(df)
```

# Hierarchical clustering

In hierarchical clustering, we use the Euclidean distance as our similarity measure between each observation (in this case, the countries in our dataset).

[either add link to or explain Euclidean distance, also add scaling part here]

## Optimal number of clusters {.tabset}

[Unlike $k$-means clustering, we don't have to specify the number of clusters beforehand. But then how many clusters should we use?]

[Describe what the `fviz_nbclust()` function does]

[Maybe try to describe these sections better? Especially gap statistic]

[See [here](https://towardsdatascience.com/how-many-clusters-6b3f220f0ef5)]

### Elbow method

-   Looks at the total within-cluster sum of squares (WSS)
-   Similar to proportion of variance explained (PVE) plot in PCA
-   The optimal number of clusters is the point where the plot appears to bend
-   Lower WSS = better

```{r}
# best = 4 clusters (not much improvement after 4 clusters)
fviz_nbclust(df, 
             hcut, 
             method = "wss")
```

### Average silhouette method

-   Determines how well each object lies within its cluster
-   Higher average silhouette = better

```{r}
# best = 2 clusters
fviz_nbclust(df, 
             hcut, 
             method = "silhouette")
```

### Gap statistic method

-   Compares the total within intra-cluster variation for different values of $k$ with randomly drawn samples
-   Choose $k$ such that the gap statistic is within 1 standard deviation of the gap statistic at $k + 1$
-   Higher gap statistic = better

```{r}
set.seed(123)
gap_stat <- clusGap(df,
                    hcut,
                    K.max = 10, # number of clusters to consider
                    B = 500)    # number of samples to bootstrap

# best = 3 clusters
fviz_gap_stat(gap_stat)
```

## Dendrograms {.tabset}

To visually show the hierarchical clusters, we can plot diagrams called *dendrograms* for each of the following clusters. We use $k = 2, 3, 4$ clusters based on the optimal number of clusters in the previous section.

[Identify changes]

### $k = 2$ clusters

```{r, fig.height=20}
df %>%
  scale() %>%                # scale and center the columns
  dist() %>%                 # get the Euclidean distances between rows             
  hclust() %>%               # apply hierarchical clustering
  as.dendrogram() %>%        # turn the cluster output into a dendrogram
  set("labels_cex", 0.4) %>% # make the font size smaller
  color_branches(k = 2) %>%  # color the branches based on the 2 clusters
  color_labels(k = 2) %>%    # color the labels based on the 2 clusters
  plot(horiz = TRUE)         # make the labels appear on the right
```

### $k = 3$ clusters

```{r, fig.height=20}
df %>%
  scale() %>%                # scale and center the columns
  dist() %>%                 # get the Euclidean distances between rows  
  hclust() %>%               # apply hierarchical clustering
  as.dendrogram() %>%        # turn the cluster output into a dendrogram
  set("labels_cex", 0.4) %>% # make the font size smaller
  color_branches(k = 3) %>%  # color the branches based on the 3 clusters
  color_labels(k = 3) %>%    # color the labels based on the 3 clusters
  plot(horiz = TRUE)         # make the labels appear on the right
```

### $k = 4$ clusters

```{r, fig.height=20}
df %>%
  scale() %>%                # scale and center the columns
  dist() %>%                 # get the Euclidean distances between rows  
  hclust() %>%               # apply hierarchical clustering
  as.dendrogram() %>%        # turn the cluster output into a dendrogram
  set("labels_cex", 0.4) %>% # make the font size smaller
  color_branches(k = 4) %>%  # color the branches based on the 4 clusters
  color_labels(k = 4) %>%    # color the labels based on the 4 clusters
  plot(horiz = TRUE)         # make the labels appear on the right
```

## Map clusters {.tabset}

The dendrograms can be difficult to read since there are a lot of countries -- `r length(row.names(df))` in total! Luckily, since we are working with countries, we can also plot the clusters onto a world map. Below are some of the functions that we'll use to make our maps.

```{r}
# perform hierarchical clustering and get k clusters
k_hclust <- function(.df, k) {
  .df %>%
    scale() %>%  # scale and center the columns
    dist() %>%   # get the Euclidean distances between rows
    hclust() %>% # apply hierarchical clustering
    cutree(k)    # separate observations into k clusters
}

# turn the cluster output into a dataframe
clust_to_df <- function(.clust) {
  .clust %>%                              # must be a named vector: clusters as values, countries as names
    cbind() %>%                           # combine countries and clusters by column
    data.frame() %>%                      # convert to data frame
    rename(cluster = 1) %>%               # rename the first column as 'cluster'
    mutate(cluster = factor(cluster)) %>% # convert the 'cluster' column into a factor
    rownames_to_column("country")         # turn the row names (countries) into a column called 'country'
}

# rename countries to be able to plot them
rename_countries <- function(.df) {
  .df %>%
    # replace original country names with new country names (important for the plot_map() function!)
    mutate(across('country', str_replace, 'Antigua and Barbuda', 'Antigua'),
           across('country', str_replace, 'Congo, Dem. Rep.', 'Democratic Republic of the Congo'),
           across('country', str_replace, 'Congo, Rep.', 'Republic of Congo'),
           across('country', str_replace, 'Cote d\'Ivoire', 'Ivory Coast'),
           across('country', str_replace, 'Kyrgyz Republic', 'Kyrgyzstan'),
           across('country', str_replace, 'Lao', 'Laos'),
           across('country', str_replace, 'Macedonia, FYR', 'North Macedonia'),
           across('country', str_replace, 'Micronesia, Fed. Sts.', 'Micronesia'),
           across('country', str_replace, 'Slovak Republic', 'Slovakia'),
           across('country', str_replace, 'St. Vincent and the Grenadines', 'Saint Vincent'),
           across('country', str_replace, 'United Kingdom', 'UK'),
           across('country', str_replace, 'United States', 'USA')) %>%
    # add separate rows for countries that were originally grouped together
    add_row(country = 'Barbuda', cluster = filter(., country == 'Antigua') %>% getElement('cluster')) %>%
    add_row(country = 'Grenadines', cluster = filter(., country == 'Saint Vincent') %>% getElement('cluster'))
}

# plots the clusters onto the world map
plot_map <- function(.df) {
  # dataframe containing information (e.g. latitude, longitude) on all countries
  world <- map_data("world")
  
  world %>%
    # (left) join 'world' dataframe with another dataframe at columns with the country names
    left_join(.df, by = c("region" = "country")) %>%
    # plot the map
    ggplot() +
      geom_polygon(aes(x = long, y = lat, fill = cluster, group = group),
                   color = "white") +
      coord_fixed(1.3) +
      theme(legend.position = "top")
}
```

If you're curious, this is how we determined what names to change.

```{r}
# dataframe containing information (e.g. latitude, longitude) on all countries
world <- map_data("world")

# get unique countries in 'df' and 'world'
unique_countries_df <- row.names(df) %>% unique() %>% sort()
unique_countries_world <- world$region %>% unique() %>% sort()

# get all countries that occur in 'df' but not in 'world'
setdiff(unique_countries_df, unique_countries_world)
```

Use the tabs to choose between different numbers of clusters obtained from hierarchical clustering. Note that some countries are not included in this dataset (shaded in gray), such as Mexico and Greenland.

### $k = 2$ clusters

```{r, out.width='150%'}
df %>%
  k_hclust(2) %>%
  clust_to_df() %>%
  rename_countries() %>%
  plot_map()
```

### $k = 3$ clusters

```{r, out.width='150%'}
df %>%
  k_hclust(3) %>%
  clust_to_df() %>%
  rename_countries() %>%
  plot_map()
```

### $k = 4$ clusters

```{r, out.width='150%'}
df %>%
  k_hclust(4) %>%
  clust_to_df() %>%
  rename_countries() %>%
  plot_map()
```

# Model-based clustering

Clusters are not necessarily strict boundaries -- for example, if a point is further away from the center of a cluster, then it may be less likely to be in that cluster. To account for this uncertainty, one assumption we can make is that the clusters follow a (multivariate) normal distribution, which is a probability density function. (More formally, this method is called Gaussian mixture models clustering.) An example of this with two variables is shown below.

<br>

<center>![Image by [Mario Castro](https://youtu.be/h7RVeO-P3zc)](../img/mb-cluster-gaussian.png){width="357"}</center>

</br>

To define such a distribution for a given set of observations, we need to find its covariance matrix. We won't get into the mathematical details here (see [this paper](https://stat.uw.edu/sites/default/files/files/reports/2012/tr597.pdf) for a more rigorous treatment), but the basic idea is that by modifying this matrix, we can create different types of clusters that are determined by three aspects:

-   **Volume:** the amount of space that each cluster contains
    -   *Equal:* all clusters have the same volume
    -   *Variable:* the clusters can have different volumes
-   **Shape:** the width of the clusters
    -   *Equal:* all clusters have the same shape (e.g. all spherical, all elliptical)
    -   *Variable:* the clusters can have different shapes
-   **Orientation:** how the clusters are positioned in space and what direction it is facing
    -   *NA:* used for spherical clusters (since rotating these clusters doesn't change the groups)
    -   *Axes:* all clusters are parallel to the axis/axes
    -   *Equal:* all clusters have the same orientation
    -   *Variable:* the clusters can have different orientations

Below is a table containing all of the possible types of clusters. The identifiers are made up of three letters, which indicate what volume, shape, and orientation the clusters have, respectively. These are labeled as follows:

-   *E* = "equal"
-   *V* = "variable"
-   *I* = "coordinate axes"

<br>

<center>![Image by [Mario Castro](https://youtu.be/h7RVeO-P3zc)](../img/mb-cluster-models-table.png){width="453"}</center>

</br>

Here are some visuals to accompany the table above.

<br>

<center>![Image by [Scrucca et al. (2016)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5096736/)](../img/mb-cluster-models-visual.jpeg){width="495"}</center>

</br>

To identify both the optimal model and number of clusters, the Bayesian Information Criterion (BIC) is used since it penalizes models with more parameters, i.e. the "general" clusters are penalized more since they have more variability (and therefore more parameters). Many combinations of different models and numbers and clusters are fit, and the combination that yields the maximum BIC is chosen as the "best" one.

<details>

<summary>**Why do we maximize the BIC? Aren't we supposed to minimize it?**</summary>

Good question! Just to give some context, the BIC allows us to compare models with different numbers of parameters. From [this article](https://towardsdatascience.com/an-intuitive-explanation-of-the-bayesian-information-criterion-71a7a3d3a5c5), we want to minimize the BIC, which is defined as $$\text{BIC} = k \ln{(n)} - 2 \ln{(\hat{L})}$$ where

-   $k$ is the number of model parameters,
-   $n$ is the number of data points, and
-   $L$ is the maximum likelihood function (i.e. how likely our data explained by a given model).

Having more parameters tends to produce more flexible models that fit the model better and thus yield a higher likelihood (i.e. higher $\hat{L}$). This means the second term $-2 \ln{(\hat{L})}$ would become more negative, which decreases the BIC.

However, we usually want to explain the data by using as few parameters as possible -- this is called the principle of parsimony. Having too many parameters can also risk overfitting the data, which may not generalize well to new data. As a result, we penalize models with more parameters. In math terms, we have a larger $k$, which makes the first term $k \ln{(n)}$ bigger, which in turn increases the BIC.

Then why do we maximize the BIC here? It turns out that in [the paper (p. 54)](https://stat.uw.edu/sites/default/files/files/reports/2012/tr597.pdf) associated with the `mclust` package (which we use to run this algorithm), the BIC is roughly defined as
$$\text{BIC} = 2 \ln{(\hat{L})} - k \ln{(n)}$$

This is essentially taking the negative of our BIC equation from earlier, hence why the algorithm tries to maximize the BIC rather than minimizing it.

</details>

## Choosing the best model {.tabset}

### Run the algorithm

We use the the `Mclust()` function in the `mclust` package to run this algorithm. The code below may take a minute or two since it is fitting all of the possible models using different numbers of clusters. The output includes the optimal model and number of clusters, along with how many observations are in each cluster.

```{r}
# run model-based clustering algorithm
mb <- Mclust(df)

# output model and number of clusters chosen
summary(mb)
```

### Classification plots

We can see how the points are classified for each pair of variables in the dataset using the chosen model and number of clusters.

```{r}
# create classification plots
plot(mb, what = "classification")
```

### BIC plot

We can also compare all of the models at different numbers of clusters. Higher BIC suggests a better model.

```{r}
# create BIC plot
plot(mb, what = "BIC")
```

## Map clusters

Like before, we plot our clusters onto the world map. Notice that we have more clusters here than when we used hierachical clustering.

```{r, out.width='150%'}
# plot map clusters
mb$classification %>%
  clust_to_df() %>%
  rename_countries() %>%
  plot_map()
```

# Density-based clustering

[Add info here]

## Choosing the best model

[Need to change `abline()` part since now the dataframe is scaled]

```{r}
install.packages("fpc")
install.packages("dbscan")
install.packages("factoextra")
```

```{r}
scale.df <- df %>% scale()
df.matrix <- as.matrix(scale.df)
kNNdistplot(df.matrix, k=10)
abline(h=2.5, col="red")
```
### eps = 4, MinPts = 5
```{r}
library("fpc")
set.seed(1)
db <- fpc::dbscan(scale.df, eps = 5, MinPts = 6)
db

library("factoextra")
fviz_cluster(db, data = scale.df, stand = FALSE,
             ellipse = FALSE, show.clust.cent = FALSE,
             geom = "point",palette = "jco", ggtheme = theme_classic())
```
### eps = 3, MinPts = 5

```{r}
library("fpc")
set.seed(1)
db <- fpc::dbscan(scale.df, eps = 3, MinPts = 5)

library("factoextra")
fviz_cluster(db, data = scale.df, stand = FALSE,
             ellipse = FALSE, show.clust.cent = FALSE,
             geom = "point",palette = "jco", ggtheme = theme_classic())
```

### eps = 6, MinPts = 5

```{r}
library("fpc")
set.seed(1)
db <- fpc::dbscan(scale.df, eps = 6, MinPts = 5)

library("factoextra")
fviz_cluster(db, data = scale.df, stand = FALSE,
             ellipse = FALSE, show.clust.cent = FALSE,
             geom = "point",palette = "jco", ggtheme = theme_classic())
```

### eps = 3, MinPts = 10

```{r}
library("fpc")
set.seed(1)
db <- fpc::dbscan(scale.df, eps = 3, MinPts = 10)

library("factoextra")
fviz_cluster(db, data = scale.df, stand = FALSE,
             ellipse = FALSE, show.clust.cent = FALSE,
             geom = "point",palette = "jco", ggtheme = theme_classic())
```

### Check PCA
```{r}
# check PCA
pr.out <- prcomp(scale.df,
                 scale =TRUE,
                 center = TRUE)

plot(pr.out$x[,1], pr.out$x[,2],
     xlab = 'PC1', ylab = 'PC2')
```
[PCA got a dense single group may be why we only got one cluster in DBscan.
ref: https://stackoverflow.com/questions/48051800/why-dbscan-clustering-returns-single-cluster-on-movie-lens-data-set]

## Map clusters

```{r, out.width='150%'}
# add country names to clusters
db_clust <- db$cluster
names(db_clust) <- row.names(df)

# plot map clusters
db_clust %>%
  clust_to_df() %>%
  rename_countries() %>%
  plot_map()
```

# Results and Anlysis
Firstly, let us look at the map clusters of Hierarchical clustering. According to the results with 2 clusters, 3 clusters, and 4 clusters, we observe that there is no significant change when we add up the number of clusters each time. Since the number of clusters are given by the optimization methods above, it shows that 2, 3, or 4 clusters will give us the best clustering results, we will not continue adding up the number of clusters over 4. Therefore, we might decide that k=2 is the optimal number of clusters we want to analyze in Hierarchical clustering. 

Focusing on the map clusters k=2 of Hierarchical clustering, we could easily find that the result mainly splits the map into two regions: a large part of African countries with some countries in South Asia (red region), and the rest of the world (blue region). Here are some examples of the countries in red region: Niger, Chad, South Africa, Nigeria, Iraq, Liberia, Afghan, etc., and some examples of the countries in blue region: Canada, the United States, Australia, India, and so on. According to the data set and the information searched online, we find that the countries in the red region has the same properties: very high child_mort, low income, low gdpp, very high total_fer, relatively low life-exp, and so on. Compared to those countries in red region,, we observe an obvious difference of those properties of the countries in blue region: generally speaking, countries in blue region have relatively better socio-economic and health conditions. It is worth mentioning that, we find that total_fer is an especially important indicator affecting the clustering results in this case. We could observe some countries in the blue region also have relatively weak socio-economic condition, like India (Income=4410, child_mort=58.8,gdpp=1350). However, those countries with weak socio-economic condition in the blue region always have relatively low total_fer, compared to pretty high average level of total_fer of the countries in red region. 

In this case, we could infer that the 2 clusters Hierarchical clustering performs well in dividing the countries into two types: Stage 1 country (red region), and Stage 2-5 country (blue region). The classification method here is known as Demographic Transition Model Stages, which is widely accepted in the social sciences because of the well-established historical correlation linking dropping fertility to social and economic development. The stage 1 country shows generally a pre-industrial society in which both birth and death rates are quite high. Birth rates and death rates are effectively in balance. The lack of food availability as well as adequate medical care or effective sanitation and hygiene means the population does not grow very much due to disease and starvation. Therefore, we conclude that Hierarchical clustering perform well in stably categorizing countries into two levels with pretty good interpretation. 
Now, looking at the Model-Based clustering with the best model of k=6. We could easily observe that Model-Based clustering perform much better than Hierarchical clustering in categorizing countries into more levels when we are adding up the number of clusters, since several regions are distinctly divided on the map. The result mainly splits the map into six regions. In the red region, it includes the countries mainly in southern Africa (South Africa, Dem. Rep. of Congo, Zambia, etc.), South Asia (India, Pakistan,etc.), and middle East (Yemen, Iraq, etc.). The properties of those countries include pretty high child_mort, low income, low gdpp, high total_fer, relatively low life-exp, and so on. In the pink region, it includes the countries mainly in Middle and Western Africa (Niger, Chad, Nigeria, Liberia, etc.), and South Asia (Afghan, Nepal, etc.). The boundary between red region and pink region is not that clear but generally we could observe that the socio-economic and health condition of pink region is even worse than the red region. Beside, the average total_fer of the countries in pink region is even higher than red region. 

# References

-   Dataset
    -   [Country data](https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data)
-   Hierarchical clustering
    -   [ISLR textbook](https://trevorhastie.github.io/ISLR/ISLR%20Seventh%20Printing.pdf)
    -   [Using the `hclust()` function](https://r-charts.com/part-whole/hclust/)
    -   [Customization for `hclust()`](https://stackoverflow.com/questions/55207216/r-rect-hclust-rectangles-too-high-in-dendogram)
    -   [Determining the optimal number of clusters](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/)
    -   [Using the `fviz_nbclust()` and `fviz_gap_stat()` functions](https://rpkgs.datanovia.com/factoextra/reference/fviz_nbclust.html)
-   Model-based clustering
    -   [Model-based clustering and Gaussian mixture model in R](https://en.proft.me/2017/02/1/model-based-clustering-r/)
    -   [Model-based clustering: an introduction to Gaussian Mixture Models (video)](https://youtu.be/h7RVeO-P3zc)
    -   [Paper for the `mclust` package](https://stat.uw.edu/sites/default/files/files/reports/2012/tr597.pdf)
-   Density-based clustering

-   Analysis
    -   [The Demographic Transition Model](https://www.intelligenteconomist.com/demographic-transition-model/)
```{r, include=FALSE}
# To put just code in our R script (runs every time this document is knit):
knitr::purl(input = 'clustering-countries.Rmd', 
            output = 'clustering-countries.R')
```
