---
title: "Clustering Analysis on Country Data"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

```{=html}
<style>
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
</style>
```
```{r, include=FALSE}
# set code chunk options
knitr::opts_chunk$set(echo = TRUE, 
                      fig.align = 'center',
                      cache = TRUE)

# allow scrolling for long code
options(width = 200)
```

# Setup

**Objective:** We want to categorize countries using socio-economic and health factors that determine the overall development of each country (from the [Kaggle description](https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data)).

We load in the necessary packages and the [country data](https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data).

```{r, message=FALSE}
# load packages
library(tidyverse)
library(dendextend)
library(factoextra)
library(mclust)

# read data
countries <- read.csv("../data/country-data.csv")

# use the countries as the row names
df <- countries %>%
  column_to_rownames(var = "country")

# show the first few rows of the dataframe
head(df)
```

# Hierarchical clustering

In hierarchical clustering we focused on using euclidian distance as our measure for the objects.We represented our hierarchical data with a dendrogram which is an upside down tree. We found that our optimal number of clusters was 3.

## Optimal number of clusters

**Elbow method**

-   Looks at the total within-cluster sum of squares (WSS)
-   Similar to proportion of variance explained (PVE) plot in PCA
-   The optimal number of clusters is the point where the plot appears to bend
-   Lower WSS = better

```{r}
# best = 4 clusters (not much improvement after 4 clusters)
fviz_nbclust(df, 
             hcut, 
             method = "wss")
```

**Average silhouette method**

-   Determines how well each object lies within its cluster
-   Higher average silhouette = better

```{r}
# best = 2 clusters
fviz_nbclust(df, 
             hcut, 
             method = "silhouette")
```

**Gap statistic method**

-   Compares the total within intra-cluster variation for different values of $k$ with randomly drawn samples
-   Choose $k$ such that the gap statistic is within 1 standard deviation of the gap statistic at $k + 1$
-   Higher gap statistic = better

```{r}
library(cluster)
set.seed(123)
gap_stat <- clusGap(df,
                    hcut,
                    K.max = 10, # number of clusters to consider
                    B = 500)    # number of samples to bootstrap

# best = 3 clusters
fviz_gap_stat(gap_stat)
```

## Dendrograms {.tabset}

To visually show the hierarchical clusters, we can plot dendrograms for each of the following clusters.

[Identify changes]

### $k = 2$ clusters

```{r, fig.height=20}
df %>%
  scale() %>%
  dist() %>%
  hclust() %>%
  as.dendrogram() %>%
  set("labels_cex", 0.4) %>%
  color_branches(k = 2) %>% 
  color_labels(k = 2) %>%
  plot(horiz = TRUE)
```

### $k = 3$ clusters

```{r, fig.height=20}
df %>%
  scale() %>%
  dist() %>%
  hclust() %>%
  as.dendrogram() %>%
  set("labels_cex", 0.4) %>%
  color_branches(k = 3) %>% 
  color_labels(k = 3) %>%
  plot(horiz = TRUE)
```

### $k = 4$ clusters

```{r, fig.height=20}
df %>%
  scale() %>%
  dist() %>%
  hclust() %>%
  as.dendrogram() %>%
  set("labels_cex", 0.4) %>%
  color_branches(k = 4) %>% 
  color_labels(k = 4) %>%
  plot(horiz = TRUE)
```

## Map clusters {.tabset}

The dendrograms can be difficult to read since there are a lot of countries -- `r length(row.names(df))` in total! Luckily, since we are working with countries, we can also plot the clusters onto a world map. Below are some of the functions that we'll use to make our maps. 

```{r}
# perform hierarchical clustering and get k clusters
k_hclust <- function(.df, k) {
  .df %>%
    scale() %>%
    dist() %>%
    hclust() %>%
    cutree(k)
}

# turn the cluster output into a dataframe
clust_to_df <- function(.clust) {
  .clust %>%
    cbind() %>% 
    data.frame() %>%
    rename(cluster = 1) %>%
    mutate(cluster = factor(cluster)) %>%
    rownames_to_column("country")
}

# rename countries to be able to plot them
rename_countries <- function(.df) {
  .df %>%
    mutate(across('country', str_replace, 'Antigua and Barbuda', 'Antigua'),
           across('country', str_replace, 'Congo, Dem. Rep.', 'Democratic Republic of the Congo'),
           across('country', str_replace, 'Congo, Rep.', 'Republic of Congo'),
           across('country', str_replace, 'Cote d\'Ivoire', 'Ivory Coast'),
           across('country', str_replace, 'Kyrgyz Republic', 'Kyrgyzstan'),
           across('country', str_replace, 'Lao', 'Laos'),
           across('country', str_replace, 'Macedonia, FYR', 'North Macedonia'),
           across('country', str_replace, 'Micronesia, Fed. Sts.', 'Micronesia'),
           across('country', str_replace, 'Slovak Republic', 'Slovakia'),
           across('country', str_replace, 'St. Vincent and the Grenadines', 'Saint Vincent'),
           across('country', str_replace, 'United Kingdom', 'UK'),
           across('country', str_replace, 'United States', 'USA')) %>%
    add_row(country = 'Barbuda', cluster = filter(., country == 'Antigua') %>% getElement('cluster')) %>%
    add_row(country = 'Grenadines', cluster = filter(., country == 'Saint Vincent') %>% getElement('cluster'))
}

# plots the clusters onto the world map
plot_map <- function(.df) {
  world <- map_data("world")
  
  world %>%
    left_join(.df, by = c("region" = "country")) %>%
    ggplot() + 
    geom_polygon(aes(x = long, y = lat, fill = cluster, group = group),
                 color = "white") +
    coord_fixed(1.3) +
    theme(legend.position = "top")
}
```

If you're curious, this is how we determined what names to change.

```{r}
# get world data
world <- map_data("world")

# get unique countries in the df and world dataframes
unique_countries_df <- row.names(df) %>% unique() %>% sort()
unique_countries_world <- world$region %>% unique() %>% sort()

# get all countries that occur in df but not in world
setdiff(unique_countries_df, unique_countries_world)
```

Use the tabs to choose between different numbers of clusters obtained from hierarchical clustering. Note that some countries are not included in this dataset (shaded in gray), such as Mexico and Greenland.

### $k = 2$ clusters

```{r, out.width='150%'}
df %>%
  k_hclust(2) %>%
  clust_to_df() %>%
  rename_countries() %>%
  plot_map()
```

### $k = 3$ clusters

```{r, out.width='150%'}
df %>%
  k_hclust(3) %>%
  clust_to_df() %>%
  rename_countries() %>%
  plot_map()
```

### $k = 4$ clusters

```{r, out.width='150%'}
df %>%
  k_hclust(4) %>%
  clust_to_df() %>%
  rename_countries() %>%
  plot_map()
```

# Model-based clustering

Clusters are not necessarily strict boundaries -- for example, if a point is further away from the center of a cluster, then it may be less likely to be in that cluster. To account for this uncertainty, one assumption we can make is that the clusters follow a (multivariate) Gaussian or normal distribution, which is a probability density function. An example of this with two variables is shown below.

<br>

<center>![[Image source](https://youtu.be/h7RVeO-P3zc)](../img/mb-cluster-gaussian.png){width="357"}</center>

</br>

To define such a distribution for a given set of observations, we need to define a covariance matrix. We won't get into the mathematical details here (see [this paper](https://stat.uw.edu/sites/default/files/files/reports/2012/tr597.pdf) for a more rigorous treatment), but the basic idea is that by modifying this matrix, we can create different types of clusters that are determined by three aspects:

-   **Volume:** the number of observations in each cluster
    -   *Equal:* all clusters have (roughly) the same number of observations
    -   *Variable:* the clusters can have varying number of observations
-   **Shape:** the width of the clusters
    -   *Equal:* all clusters have the same shape (e.g. all spherical, all elliptical)
    -   *Variable:* the clusters can have different shapes
-   **Orientation:** how the clusters are positioned in space and what direction it is facing
    -   *NA:* used for spherical clusters (since rotating these clusters doesn't change the groups)
    -   *Axes:* all clusters are parallel to the axis/axes
    -   *Equal:* all clusters have the same orientation
    -   *Variable:* the clusters can have different orientations

Below is a table containing all of the possible types of clusters. The identifiers are made up of three letters, which indicate what volume, shape, and orientation the clusters have, respectively. These are labeled as follows:

-   *E* = "equal"
-   *V* = "variable"
-   *I* = "coordinate axes"

<br>

<center>![[Image source](https://youtu.be/h7RVeO-P3zc)](../img/mb-cluster-models-table.png){width="453"}</center>

</br>

Here are some visuals to accompany the table above (note that not all of the models are shown).

<br>

<center>![[Image source](https://stat.uw.edu/sites/default/files/files/reports/2012/tr597.pdf)](../img/mb-cluster-models-visual.png){width="433"}</center>

</br>

To identify both the optimal model and number of clusters, the Bayesian Information Criterion (BIC) is used since it penalizes models with more parameters, i.e. the "general" clusters are penalized more since they have more variability (and therefore more parameters). Many combinations of different models and numbers and clusters are fit, and the combination that yields the maximum BIC is chosen as the "best" one.

## Choosing the best model {.tabset}

### Run the algorithm

Run the code below (it may take a minute or two since it is trying all of the possible models using different numbers of clusters). The output includes the optimal model and number of clusters, along with how many observations are in each cluster.

```{r}
mb <- Mclust(df)
summary(mb)
```

### Classification plots

We can see how the points are classified for each pair of variables in the dataset.

```{r}
plot(mb, what = "classification")
```

### BIC plot

We can also compare all of the models at different numbers of clusters. Higher BIC suggests a better model.

```{r}
plot(mb, what = "BIC")
```

## Map clusters

Like before, we plot our clusters onto the world map. Notice that we have more clusters here than when we used hierachical clustering.

```{r, out.width='150%'}
mb$classification %>%
  clust_to_df() %>%
  rename_countries() %>%
  plot_map()
```

# Density-based clustering

[Add info here]

## Choosing the best model

```{r}
library(dbscan)
str(df)
```

```{r}
df.matrix <- as.matrix(df)
kNNdistplot(df.matrix, k=6)
abline(h=5000, col="red")
```

```{r}
set.seed(1)
db <- dbscan(df, eps = 5000, minPts = 6)
db
summary(db)
```

## Map clusters

```{r}
fviz_cluster(db, df, geom = 'point')
```

```{r}
hullplot(df.matrix, db$cluster)
```

```{r, include=FALSE}
# plot(db, df, main = "DBScan")
```

```{r, out.width='150%'}
# add country names to clusters
db_clust <- db$cluster
names(db_clust) <- row.names(df)

# plot map clusters
db_clust %>%
  clust_to_df() %>%
  rename_countries() %>%
  plot_map()
```

# References

-   Dataset
    -   [Country data](https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data)
-   Hierarchical clustering
    -   [ISLR textbook](https://trevorhastie.github.io/ISLR/ISLR%20Seventh%20Printing.pdf)
    -   [Using the `hclust()` function](https://r-charts.com/part-whole/hclust/)
    -   [Customization for `hclust()`](https://stackoverflow.com/questions/55207216/r-rect-hclust-rectangles-too-high-in-dendogram)
    -   [Determining the optimal number of clusters](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/)
    -   [Using the `fviz_nbclust()` and `fviz_gap_stat()` functions](https://rpkgs.datanovia.com/factoextra/reference/fviz_nbclust.html)
-   Model-based clustering
    -   [Model-based clustering and Gaussian mixture model in R](https://en.proft.me/2017/02/1/model-based-clustering-r/)
    -   [Model-based clustering: an introduction to Gaussian Mixture Models (video)](https://youtu.be/h7RVeO-P3zc)
-   Density-based clustering

To put just code in our R script:
knitr::purl(input = 'clustering-countries.Rmd', output = 'clustering-countries.R')
