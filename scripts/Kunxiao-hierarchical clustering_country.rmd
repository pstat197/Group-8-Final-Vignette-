---
title: "Kunxiao Gao-hierarchical clustering"
author: "Kunxiao Gao"
date: "2022-11-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ROCR)
library(ggridges)
library(dendextend)
library(e1071)
```

```{r}
##summary the information of the dataframe
summary(data)
##Drop the country column
data <- data[-1]
```

```{r}
##Run PCA on the scaled and centered data without the last column(Health level)
pr.out <- prcomp(data[-9],scale.=TRUE,center = TRUE)
##The `prcomp()` function also outputs the standard deviation of each principal component.
# The variance explained by each principal component is obtained by squaring these:
pr.var=pr.out$sdev^2
##To compute the proportion of variance explained by each principal component, we simply divide the variance explained by each principal component by the total variance explained by all eight principal components:
pve=pr.var/sum(pr.var)
plot(pve, xlab="Principal Component", 
     ylab="Proportion of Variance Explained ", type='b')
plot(cumsum(pve), xlab="Principal Component ", 
     ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type='b')
```


```{r dendogram, fig.height=8}
##To perform hierarchical clustering, we can use function `hclust()` by passing the distance matrix into it. By default, `hclust()` uses complete linkage (defaulting `method="complete"`). We can use single linkage (by specifying `method="single"`) and average linkage (`method="average"`) too. Naturally, the first step is calculating a distance matrix. Here we use the default method:"Euclidean". The Hierarchical clustering is shown below.
data.dist = dist(data)
set.seed(123)
data.hclust.complete = hclust(data.dist)
```


```{r fig.align="center", fig.height=10, fig.width=5}
## dendrogram: branches colored by 5 groups
dend1 = as.dendrogram(data.hclust.complete)
# color branches and labels by 5 clusters
dend1 = color_branches(dend1, k=5)
dend1 = color_labels(dend1, k=5)
# change label size 
dend1 = set(dend1, "labels_cex", 0.5)
# add true labels to observations
dend1 = set_labels(dend1, labels=data$health_level[order.dendrogram(dend1)])
# plot the dendrogram
plot(dend1, horiz=T, main = "Dendrogram colored by five clusters.complete")
```

```{r dendogram, fig.height=8}
##Now we try the average linkage (`method="average") and use the default method:"Euclidean". The Hierarchical clustering is shown below.
set.seed(123)
data.hclust.average = hclust(data.dist,method = "average")
```


```{r fig.align="center", fig.height=10, fig.width=5}
## dendrogram: branches colored by 5 groups
dend1 = as.dendrogram(data.hclust.average)
# color branches and labels by 5 clusters
dend1 = color_branches(dend1, k=5)
dend1 = color_labels(dend1, k=5)
# change label size 
dend1 = set(dend1, "labels_cex", 0.5)
# add true labels to observations
dend1 = set_labels(dend1, labels=data$health_level[order.dendrogram(dend1)])
# plot the dendrogram
plot(dend1, horiz=T, main = "Dendrogram colored by five clusters.average")
```

```{r}
##Now we try the single linkage (`method="single") and use the default method:"Euclidean". The Hierarchical clustering is shown below.
set.seed(123)
data.hclust.single = hclust(data.dist,method = "single")
```

```{r fig.align="center", fig.height=10, fig.width=5}
## dendrogram: branches colored by 5 groups
dend1 = as.dendrogram(data.hclust.single)
# color branches and labels by 5 clusters
dend1 = color_branches(dend1, k=5)
dend1 = color_labels(dend1, k=5)
# change label size 
dend1 = set(dend1, "labels_cex", 0.5)
# add true labels to observations
dend1 = set_labels(dend1, labels=data$health_level[order.dendrogram(dend1)])
# plot the dendrogram
plot(dend1, horiz=T, main = "Dendrogram colored by five clusters.single")
```

